<p>Now that we understand the MCP architecture and have experimented with the Playwright MCP server, let's create our own custom MCP server from scratch. We'll build a simple Echo MCP server that will help us understand the core components of an MCP server.</p>

<h5>Understanding the Tools</h5>

<p>For this stage, we'll use two main tools:</p>

<p><strong>MCP Python Package:</strong></p>
<ul>
  <li><a href="https://github.com/modelcontextprotocol/python-sdk">Official Python SDK for building MCP servers</a></li>
  <li>Provides a standardized way to expose resources, tools, and prompts to LLMs</li>
  <li>Handles the protocol communication so we can focus on implementing functionality</li>
</ul>

<p><strong>uv Package Manager:</strong></p>
<ul>
  <li><a href="https://docs.astral.sh/uv/getting-started/installation/">Modern Python package installer and resolver</a></li>
  <li>Significantly faster than pip for installing packages</li>
  <li>Better dependency resolution and virtual environment management</li>
</ul>

<p>Using these tools together will allow us to quickly create a functional MCP server with minimal boilerplate code.</p>

<h5>Setting Up the Environment</h5>

<p>Let's start by installing the required packages:</p>

<callout label="Execute prompt using Code mode in Junie" type="composer">
Instal mcp[cli] package.
Use only uv package manager to install the mcp package.
Install uv if needed. 
Use virtual environment. 
Use python 3.12.
Create requirements.txt.
</callout>

<checkable-item title="Packages are installed"></checkable-item>

<h5>Creating the Echo MCP Server</h5>

<p>Now, let's create our Echo MCP server. This will be a simple server that demonstrates the main components of an MCP server: resources and tools.</p>

<callout label="Execute prompt using Code mode in Junie" type="composer">Create a file named echo_server.py with the following content:

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Echo")

@mcp.resource("echo://{message}")
def echo_resource(message: str) -> str:
    """Echo a message as a resource"""
    return f"Resource echo: {message}"

@mcp.tool()
def echo_tool(message: str) -> str:
    """Echo a message as a tool"""
    return f"Tool echo: {message}"

if __name__ == "__main__":
    mcp.run(transport='stdio')
</callout>

<checkable-item title="<code>echo_server.py</code> file created"></checkable-item>
<checkable-item title="Content of <code>echo_server.py</code> file is the same as in the prompt"></checkable-item>

<h5>Understanding the MCP Components</h5>

<p>There are core primitives that we implement in our server:</p>

<p><strong>1. Resources (@mcp.resource):</strong></p>
<ul>
  <li>How you expose data to LLMs</li>
  <li>Similar to GET endpoints in a REST API</li>
  <li>Provide data without performing significant computation</li>
  <li>Should not have side effects</li>
  <li>Accessed via URI patterns like "echo://{message}"</li>
  <li>In our example, the resource returns the message with a "Resource echo:" prefix</li>
</ul>

<p><strong>2. Tools (@mcp.tool):</strong></p>
<ul>
  <li>Let LLMs take actions through your server</li>
  <li>Expected to perform computation and have side effects</li>
  <li>Called directly by the AI when it needs to execute a specific function</li>
  <li>Examples include making API calls, updating data, or triggering workflows</li>
  <li>In our example, the tool returns the message with a "Tool echo:" prefix</li>
</ul>

<p>The MCP Python SDK makes it easy to implement these components using decorators, as shown in our echo server example.</p>

<p>In the next step we will add our MCP server to the Claude Desktop configuration and test it.</p> 